 # TỔNG QUAN #

$${\color{green}Tổng \space quan}$$

Kiểm tra các file trên máy chủ nếu chúng có tiết lộ các thông tin nhạt cảm. Các file này có thể có thông tin về cấu trúc web và cách chúng hoạt động

$${\color{green}Vấn \space đề}$$

- Rủi ro bảo mật: kẻ xấu có thể lợi dụng để khai thác các thông tin mật
- Đường dẫn ẩn: Chúng có thể tiết lộ những đường dẫn ẩn của trang web

 # ROBOTS #

$${\color{green}Định \space nghĩa}$$

Web Spiders, Robots, hoặc trình thu thập là các chương trình tự động sẽ đi đến các trang web và đường link để lấy dữ liệu. Chúng thường được các công cụ tìm kiếm sử dụng để lập các chỉ mục web hoặc bởi các công cụ bảo mật để phân tích ứng dụng web 

$${\color{green}Cách \space hoạt \space động}$$

1.Robots sẽ bắt đầu từ việc truy cập vào trang web
2. Tiếp đến chúng sẽ tự động đi thưo các hyperlinks từ trang này sang trang khác trong cùng 1 website
3. Chúng thu thập thông tin qua các trang này

$${\color{green}Ví \space dụ}$$

`robots.txt` file từ [Facebook](https://facebook.com/robots.txt)

>User-agent: Amazonbot
>Disallow: /
>
>User-agent: Applebot-Extended
>Disallow: /

Phần này của file `robots.txt` cung cấp các chỉ thị cho các bot hoặc trình thu thập dữ liệu, yêu cầu chúng không được truy cập bất kỳ phần nào trong website

Ví dụ, `User-Agent: Amazonbot` là con bot của Google trong khi `User-Agent: Applebot-Extended` là trình thu thập dữ liệu của Microsoft. User-Agent: * trong ví dụ trên không được phép truy cập vào bất kì phần nào trong trang

>Disallow: /*/plugins/*
>Disallow: /?*next=
>Disallow: /a/bz?
>Disallow: /ajax/
>Disallow: /album.php

`/*/plugins/*` ( chặn truy cập vào bất kỳ URL nào có chứa /plugins/ trong bất kỳ đoạn nào )

`/?*next=` ( chặn các URL có tham số truy vấn  `next=`)

`/a/bz?` ( chặn các URL trùng khớp chính xác với đường dẫn này)

`/ajax/` ( chặn truy cập các URL bắt đầu bằng `/ajax/`)

`/album.php` ( chặn tệp tin )

- File `robots.txt` được lấy từ thư mục gốc của máy chủ web . Ví dụ, để lấy tệp `robots.txt` từ www.google.com bằng cách sử dụng `wget` hoặc `curl`:

- Đầu tiên, tải xuống tệp bằng lệnh 'curl'
>curl -O -Ss http://www.google.com/robots.txt

- Sử dụng `cat` để xem nội dung file:
>cat robots.txt

![image](https://github.com/user-attachments/assets/413c08fa-14bf-42ae-8971-bcfda320beb6)

# PHÂN TÍCH FILE ROBOTS.TXT BẰNG GOOGLE WEBMASTER TOOLS #
 
Ta có thể sử dụng chức năng “Analyze robots.txt” của Google để phân tích trang web [Google Webmaster Tools](https://search.google.com/search-console/welcome?hl=en&utm_source=wmx&utm_medium=deprecation-pane&utm_content=home). 

1.Đăng nhập vào Google Webmaster Tools bằng tài khoản Google của bạn.

2.Nhập URL bạn muốn phân tích.

3.Chọn phương pháp phân tích và làm theo hướng dẫndẫn.

# THẺ META #

$${\color{green}Định \space nghĩa}$$

Thẻ META thường được đặt trong phần `<head>` của HTML. Chúng cung cấp thông tin quan trọng về trang web cho các trình duyệt, công cụ tìm kiếm và các trình thu thập dữ liệu khác. Việc thẻ META được sử dụng nhất quán trên toàn bộ trang web giúp đảm bảo các công cụ tìm kiếm, trình thu thập dữ liệu có thể hiểu  và lập chỉ mục chính xác, bất kể chúng truy cập trang web từ đâu.

$${\color{green}Ý \space chính}$$

**1. Tính nhất quán trên trang web :**

- Cần đảm bảo rằng các thẻ META như `description`, `keyword`, `robots` có sự đồng nhất trên toàn bộ trang web để tránh sự 
- Ensure that META tags such as `description`, `keywords`, and `robots` are consistent across your site to avoid discrepancies in how pages are indexed and presented.
- This consistency is particularly important if a crawler starts from a deep link rather than the web root. Consistent META tags ensure that the same rules apply throughout the site.

**2. Robots META Tag:**

- The `robots` META tag provides directives to web crawlers on how to index and follow links on the page.

![image](https://github.com/user-attachments/assets/a3f5f747-c435-4359-8609-6e752ce02685)

- Common values for the `robots` tag include:
  - `index`, `follow`: Allow indexing and following links.
  - `noindex`, `follow`: Do not index the page but follow links.
  - `index`, `nofollow`: Index the page but do not follow links.
  - `noindex`, `nofollow`: Do not index the page and do not follow links.
    
**3. Robots.txt vs. META Tags:**

- Both `robots.txt` and META tags control crawling and indexing, but they serve different purposes:
  - robots.txt: Used to control access to parts of your site for all crawlers, generally applied at the site level.
  - META Tags: Provide page-specific directives and can be used to override robots.txt settings for individual pages.

**4.Miscellaneous META Information Tags:**

Organizations often embed informational META tags in web content to support various technologies such as screen readers, social networking previews, search engine indexing, etc. Such meta-information can be of value to testers in identifying technologies used, and additional paths/functionality to explore and test.

The following meta information was retrieved from [The New York Times](https://www.nytimes.com/2024/08/27/books/review/at-war-with-ourselves-hr-mcmaster.html) via View Page Source:

![image](https://github.com/user-attachments/assets/31125690-d782-4d8c-9a27-c40a04a593d1)


# SITEMAPS #

A sitemap is a file where a developer or organization can provide information about the pages, videos, and other files offered by the site or application, and the relationship between them. Search engines can use this file to more intelligently explore your site. Testers can use `sitemap.xml` files to learn more about the site or application to explore it more completely.

>Sitemap là một tệp tin mà nhà phát triển hoặc tổ chức có thể cung cấp thông tin về các trang, video và các tệp tin khác của trang web hoặc ứng dụng, cũng như mối quan hệ giữa chúng. Các công cụ tìm kiếm có thể sử dụng tệp tin này để khám phá trang web của bạn một cách thông minh hơn. Các kiểm thử viên cũng có thể sử dụng các tệp tin `sitemap.xml` để tìm hiểu thêm về trang web hoặc ứng dụng và khám phá nó một cách đầy đủ hơn.

For example, this is the `sitemap.xml` of Google

![image](https://github.com/user-attachments/assets/e1a14c09-1285-455b-a1d3-b2fc120496f0)

# SECURITY.TXT #

Security.txt is an accepted standard for website security information that allows security researchers to report security vulnerabilities easily. There are multiple reasons this might be of interest in testing scenarios, including but not limited to:
- Identifying further paths or resources to include in discovery/analysis.
- Open Source intelligence gathering.
- Finding information on Bug Bounties, etc.
- Social Engineering.

The file may be present either in the root of the webserver or in the `.well-known/` directory. Ex:

- `https://example.com/security.txt`
- `https://example.com/.well-known/security.txt`

Here is a real world example retrieved from [Facebook](https://www.facebook.com/security.txt)

![image](https://github.com/user-attachments/assets/5a5217ea-3d95-4576-b15f-d3f450e9778b)

# HUMANS.TXT #

`humans.txt` is an initiative for knowing the people behind a website. It takes the form of a text file that contains information about the different people who have contributed to building the website. See humanstxt for more info. This file often (though not always) contains information for career or job sites/paths.

Here is a real world example retrieved from [Google](https://www.google.com/humans.txt)

![image](https://github.com/user-attachments/assets/4da4c3bb-87c5-494a-8ca8-58bb4d57d4ec)

# OTHER .well-known INFORMATION SOURCES #

There are other RFCs and Internet drafts which suggest standardized uses of files within the `.well-known/` directory.

# TOOLS #

- Browser (View Source or Dev Tools functionality)
- curl
- wget
- Burp Suite
- ZAP


